<!DOCTYPE html>

<HTML>
<HEAD>
  <META content="IE=5.0000" http-equiv="X-UA-Compatible">
  <META name="description" content="Zhiliang Peng's home page"> 
  <META http-equiv="Content-Type" content="text/html; charset=gb2312">
  <LINK href="files/doc.css" 
    rel="stylesheet" type="text/css"> 
  <TITLE>Zhiliang Peng</TITLE> 
  <META name="GENERATOR" content="MSHTML 11.00.10570.1001">
</HEAD>


<BODY> 
  <DIV id="layout-content" style="margin-top: 25px;">
  <TABLE>
    <TBODY>
    <TR>
      <TD width="670">
        <DIV id="toptitle">
        <H1>Zhiliang Peng &nbsp;</H1></DIV>
        <H3>Ph.D. candidate</H3>
        <P>Room 330, Academy 2 Building 
        <BR>School of Electronic, Electrical and Communication Engineering
        <BR>University of Chinese Academy of Sciences
        <BR>Beijing, China, 101408.
        <BR>
        <BR> Email:  
        <A href="mailto:pengzhiliang19@mails.ucas.ac.cn"> pengzhiliang19@mails.ucas.ac.cn</A>; 
        <BR> Github: 
        <A href="https://github.com/pengzhiliang">https://github.com/pengzhiliang</A>;
        <BR> Google scholar:
        <A href="https://scholar.google.com/citations?user=-X1tyN0AAAAJ&hl=en">https://scholar.google.com</A>
        <BR><BR></P>
      </TD>
      <TD>
        <IMG width="150" src="files/person_photo.jpg" border="0">
      </TD>
    </TR>
    <TR></TR></TBODY>
  </TABLE>
  <DIV id="layout-content" style="margin-top: 25px;">


  <H2>Biography</H2>
  <P> I am a Ph.D. candidate of <A href="https://ucassdl.cn/l">PRISDL</A> in the <A href="http://eece.ucas.ac.cn/index.php/en/">School of Electronic, Electrical and Communication Engineering</A>, 
    <A href="http://english.ucas.ac.cn/">University of Chinese Academy of Sciences </A>, 
    advised by <A href="http://people.ucas.ac.cn/~0007279?language=en">Prof. Qixiang Ye</A>. 
    I got a B.E. degree in Huazhong University of Science and Technology, Wuhan in June 2019.      
  </P>

  <P> Presently, I am an intern at Microsoft Research, working with <A href="http://dong.li/">Dr. Li Dong</A>.
  </P>

  <P>My research interests include computer vision and deep learning, specifically for representation learning.</P>

  <!-- <H2>News</H2>
  <UL>
    <LI>
      Our paper "Learning to Match Anchors for Visual Object Detection" has been accepted by IEEE TPAMI 2021</A>.
    </LI>
    <LI>
      New version of FreeAnchor code is released <A href="https://github.com/zhangxiaosong18/FreeAnchor">[here]</A>.
    </LI>
  </UL> -->

  <H2>Publications</H2>
    <table class="pub_table">
    <!-- <tbody> -->

      <tr>
        <td class="pub_td1"><img src="files/PaperFig/kosmos2.png" class="papericon"></td>
        <td 
          class="pub_td2">&ast;<u>Zhiliang Peng</u>, &ast;Wenhui Wang, &ast;Li Dong, Yaru Hao, Shaohan Huang, Shuming Ma, Furu Wei
          <br><b>Kosmos-2: Grounding Multimodal Large Language Models to the World</b>
          <br>
          [<a href="https://arxiv.org/abs/2306.14824">Paper</a>]
          [<a href="https://aka.ms/kosmos-2-demo">Demo</a>]
          [<a href="https://github.com/microsoft/unilm/tree/master/kosmos-2">Code</a>]
          <img src="https://img.shields.io/github/stars/microsoft/unilm?style=social">
          <br>
        </td>
      </tr>

      <tr>
        <td class="pub_td1"><img src="files/PaperFig/g2sd.png" class="papericon"></td>
        <td 
          class="pub_td2">&ast;Wei Huang, &ast;<u>Zhiliang Peng</u>, Li Dong, Furu Wei, Jianbin Jiao, Qixiang Ye
          <br><b>Generic-to-Specific Distillation of Masked Autoencoders</b>
          <br>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2023
          <br>
          [<a href="https://arxiv.org/abs/2302.14771">Paper</a>]
          [<a href="https://github.com/pengzhiliang/G2SD">Code</a>]
          <img src="https://img.shields.io/github/stars/pengzhiliang/G2SD?style=social">
          <br>
        </td>
      </tr>

      <tr>
        <td class="pub_td1"><img src="files/PaperFig/Conformer_tpami.png" class="papericon"></td>
        <td 
          class="pub_td2"><u>Zhiliang Peng</u>, Zonghao Guo, Wei Huang, Yaowei Wang, Lingxi Xie, Jianbin Jiao, Qixiang Ye
          <br><b>Conformer: Local features coupling global representations for visual recognition and detection</b>
          <br>IEEE Transactions on Pattern Analysis and Machine Intelligence, 2023
          <br>
          [<a href="https://doi.org/10.1109/TPAMI.2023.3243048">Paper</a>]
          [<a href="https://github.com/pengzhiliang/Conformer">Code</a>]
          <img src="https://img.shields.io/github/stars/pengzhiliang/Conformer?style=social">
          <br>
        </td>
      </tr>

      <tr>
        <td class="pub_td1"><img src="files/PaperFig/maskdistill.png" class="papericon"></td>
        <td 
          class="pub_td2"><u>Zhiliang Peng</u>, Li Dong, Hangbo Bao, Qixiang Ye, Furu Wei
          <br><b>A Unified View of Masked Image Modeling</b>
          <br> Transactions on Machine Learning Research, 2023
          <br>
          [<a href="https://openreview.net/pdf?id=wmGlMhaBe0">Paper</a>]
          [<a href="https://github.com/microsoft/unilm/blob/master/unimim">Code</a>]
        </td>
      </tr>

      <tr>
        <td class="pub_td1"><img src="files/PaperFig/magneto.png" class="papericon"></td>
        <td 
          class="pub_td2">Hongyu Wang, Shuming Ma, Shaohan Huang, Li Dong, Wenhui Wang, <u>Zhiliang Peng</u>, Yu Wu, Payal Bajaj, Saksham Singhal, Alon Benhaim, Barun Patra, Zhun Liu, Vishrav Chaudhary, Xia Song, Furu Wei
          <br><b>Foundation Transformers</b>
          <br>International Conference on Machine Learning, 2023
          <br>
          [<a href="https://arxiv.org/abs/2210.06423">arXiv preprint</a>]
        </td>
      </tr>

      <tr>
        <td class="pub_td1"><img src="files/PaperFig/beit3.png" class="papericon"></td>
        <td 
          class="pub_td2">Wenhui Wang, Hangbo Bao, Li Dong, Johan Bjorck, <u>Zhiliang Peng</u>, Qiang Liu, Kriti Aggarwal, Owais Khan Mohammed, Saksham Singhal, Subhojit Som, Furu Wei
          <br><b>Image as a Foreign Language: BEiT Pretraining for All Vision and Vision-Language Tasks</b>
          <br>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2023
          <br>
          [<a href="https://arxiv.org/abs/2208.10442">Paper</a>]
          [<a href="https://github.com/microsoft/unilm/blob/master/beit3">Code</a>]
          <img src="https://img.shields.io/github/stars/microsoft/unilm?style=social">
        </td>
      </tr>

      <tr>
        <td class="pub_td1"><img src="files/PaperFig/beit2.png" class="papericon"></td>
        <td 
          class="pub_td2"><u>Zhiliang Peng</u>, Li Dong, Hangbo Bao, Qixiang Ye, Furu Wei
          <br><b>BEiT v2: Masked Image Modeling with Vector-Quantized Visual Tokenizers</b>
          <br>
          [<a href="https://arxiv.org/abs/2208.06366">arXiv preprint</a>]
          [<a href="http://aka.ms/beit2">Code</a>]
          <img src="https://img.shields.io/github/stars/microsoft/unilm?style=social">
        </td>
      </tr>

      <tr>
        <td class="pub_td1"><img src="files/PaperFig/imTED.png" class="papericon"></td>
        <td 
          class="pub_td2">Xiaosong Zhang, Feng Liu, <u>Zhiliang Peng</u>, Zonghao Guo, Fang Wan, Xiangyang Ji, Qixiang Ye
          <br><b>Integral Migrating Pre-trained Transformer Encoder-decoders for Visual Object Detection</b>
          <br>
          [<a href="https://arxiv.org/abs/2205.09613">arXiv preprint</a>]
        </td>
      </tr>


      <tr>
        <td class="pub_td1"><img src="files/PaperFig/Conformer.png" class="papericon"></td>
        <td 
          class="pub_td2"><u>Zhiliang Peng</u>, Wei Huang, Shanzhi Gu, Lingxi Xie, Yaowei Wang, Jianbin Jiao, Qixiang Ye
          <br><b>Conformer: Local features coupling global representations for visual recognition</b>
          <br>Proceedings of the IEEE/CVF International Conference on Computer Vision, 2021
          <br>
          [<a href="https://openaccess.thecvf.com/content/ICCV2021/html/Peng_Conformer_Local_Features_Coupling_Global_Representations_for_Visual_Recognition_ICCV_2021_paper.html">Paper</a>]
          [<a href="https://github.com/pengzhiliang/Conformer">Code</a>]
          <img src="https://img.shields.io/github/stars/pengzhiliang/Conformer?style=social">
          <br>
        </td>
      </tr>

      <tr>
        <td class="pub_td1"><img src="files/PaperFig/tscam.png" class="papericon"></td>
        <td 
          class="pub_td2">Wei Gao, Fang Wan, Xingjia Pan, <u>Zhiliang Peng</u>, Qi Tian, Zhenjun Han, Bolei Zhou, Qixiang Ye
          <br><b>TS-CAM: Token Semantic Coupled Attention Map for Weakly Supervised Object Localization</b>
          <br>Proceedings of the IEEE/CVF International Conference on Computer Vision, 2021
          <br>
          [<a href="https://openaccess.thecvf.com/content/ICCV2021/html/Gao_TS-CAM_Token_Semantic_Coupled_Attention_Map_for_Weakly_Supervised_Object_ICCV_2021_paper.html">Paper</a>]
          [<a href="https://github.com/vasgaowei/TS-CAM">Code</a>]
          <img src="https://img.shields.io/github/stars/vasgaowei/TS-CAM?style=social">
          <br>
        </td>
      </tr>

      <tr>
        <td class="pub_td1"><img src="files/PaperFig/LDA.png" class="papericon"></td>
        <td 
          class="pub_td2"><u>Zhiliang Peng</u>, Wei Huang, Zonghao Guo, Xiaosong Zhang, Jianbin Jiao, Qixiang Ye
          <br><b>Long-tailed Distribution Adaptation</b>
          <br>Proceedings of the 29th ACM International Conference on Multimedia, 2021
          <br>
          <!-- [<a href="https://dl.acm.org/doi/10.1145/3474085.3475479">Paper</a>] -->
          [<a href="https://arxiv.org/abs/2110.02686">Paper</a>]
          [<a href="https://github.com/pengzhiliang/LDA">Code</a>]
          <br>
        </td>
      </tr>
    <!-- </tbody> -->
    </table>

    <!-- <br>
    <H2>Awards</H2>
        <LI>	Excellent Student Scholarship, Chinese Academy of Sciences, 2020.  </LI> -->
  <H2>Github Statistics</Source></H2>
      <td class="pub_td1"><img src="https://github-readme-stats.vercel.app/api?username=pengzhiliang&show_icons=true&include_all_commits=true&title_color=2c86ea&icon_color=2c86ea&text_color=00c800&bg_color=00000000"></td>
    
  
  <br> <br> 
  <H2>Statistics</H2>
  <script type="text/javascript" src="//rf.revolvermaps.com/0/0/8.js?i=5063gq35g0n&amp;m=0&amp;c=ff0000&amp;cr1=ffffff&amp;f=arial&amp;l=33" async="async"></script>

</BODY>
</HTML>
